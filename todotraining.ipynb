{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600223641974",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   label                                       text\n0      1    小张，你明天需要把产品方案输出，然后给小王。小王后天把交互做一下，大后天评审。\n1      1                            我明天应该可以把这个文档搞定。\n2      1  统计异常数据那么简单的事情，怎么可能要3天时间呢！今天下班前，务必把这个事情搞定！\n3      1             李总说的这个想法，一定要在8月10日之前形成一个成熟的方案。\n4      1                  走廊里的空箱子太多了，小张你明天把它们全部处理掉。",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>小张，你明天需要把产品方案输出，然后给小王。小王后天把交互做一下，大后天评审。</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>我明天应该可以把这个文档搞定。</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>统计异常数据那么简单的事情，怎么可能要3天时间呢！今天下班前，务必把这个事情搞定！</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>李总说的这个想法，一定要在8月10日之前形成一个成熟的方案。</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>走廊里的空箱子太多了，小张你明天把它们全部处理掉。</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./datasets/todo_classification.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(130, 2)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "有意义文本：65\n无意义文本：65\n"
    }
   ],
   "source": [
    "print('有意义文本：%d' %data[data.label==1].shape[0])\n",
    "print('无意义文本：%d' %data[data.label==0].shape[0])\n",
    "# for name,group in data.groupby(data.columns[0]):\n",
    "#     print(name,len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 使用jieba库制作分词列表wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/vz/l8r_5knx1g3c4yz2yq3mmjnw0000gn/T/jieba.cache\nLoading model cost 1.916 seconds.\nPrefix dict has been built successfully.\n文本分词使用时间： 1.9839060306549072\n"
    }
   ],
   "source": [
    "import jieba\n",
    "import time\n",
    "\n",
    "wordlist = []\n",
    "startT = time.time()\n",
    "for row in data['text']:\n",
    "    # splitrow = jieba.cut(row,True)\n",
    "    words = [i for i in jieba.cut(row,True) if i not in ['，','。','！','？','、','.']]\n",
    "    wordlist.append(words)\n",
    "print('文本分词使用时间：',time.time()-startT)\n",
    "# print(wordlist[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 保存分词结果至本地txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtFilepath = './datasets/todotrain.txt'\n",
    "with open(txtFilepath,'w',encoding='utf-8') as fp:\n",
    "    for sentence in wordlist:\n",
    "        fp.write(' '.join(sentence))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 从本地txt文件加载分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtFilepath = './datasets/todotrain.txt'\n",
    "with open(txtFilepath,'r',encoding='utf-8') as fp:\n",
    "    wordlist = [k.strip().split(' ') for k in fp.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['小张', '你', '明天', '需要', '把', '产品', '方案', '输出', '然后', '给', '小', '王', '小', '王后', '后天', '把', '交互', '做', '一下', '大后天', '后天', '评审'], ['我', '明天', '应该', '可以', '把', '这个', '文档', '搞定']]\n"
    }
   ],
   "source": [
    "print(wordlist[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. word2vec词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 实例化word2vec对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "形成word2vec模型共花费1.23秒\n"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import time\n",
    "startTime = time.time()\n",
    "sentences = word2vec.Text8Corpus(u'./datasets/todotrain.txt')\n",
    "word2vec_model = word2vec.Word2Vec(sentences, size=500, iter=10, min_count=1)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "startTime = time.time()\n",
    "word2vec_model = Word2Vec(wordlist, size=500, iter=10, min_count=1)\n",
    "# sentences = wordlist\n",
    "# sentences：可以是一个list，对于大语料集，建议使用BrownCorpus，Text8Corpus或lineSentence构建\n",
    "# size：是指特征向量的维度，默认为100。大的size需要更多的训练数据，但是效果会更好，推荐值为几十到几百\n",
    "# min_count：可以对字典做截断，词频少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "usedTime = time.time() - startTime\n",
    "print('形成word2vec模型共花费%.2f秒' %usedTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 通过word2vec对象的most_similar方法获取词义相近的次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('我', 0.6757177114486694),\n ('的', 0.6727242469787598),\n ('公司', 0.6485905647277832),\n ('要', 0.6459726095199585),\n ('了', 0.6444182991981506),\n ('业务', 0.6415858864784241),\n ('及', 0.6383931636810303),\n ('和', 0.6368772387504578),\n ('一下', 0.6359129548072815),\n ('你', 0.6347568035125732)]"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('周一')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1.34954142e-04  5.52984420e-04  1.05477033e-04 -2.37750093e-04\n  2.18364119e-04  9.17987665e-04  5.07820398e-04  8.31887010e-04\n -9.82514699e-04  3.03041481e-04  6.45970169e-04  2.04151205e-04\n  5.32217324e-04 -3.29144154e-04  6.42439350e-04  3.27744434e-04\n -8.36458639e-04  7.76318484e-04 -3.88341345e-04  7.43025914e-04\n  4.14214563e-04 -2.82781984e-04 -1.62804063e-04 -4.72190877e-04\n  5.93135890e-04  1.94971784e-04  4.12259949e-04 -5.60372660e-04\n  6.22889493e-04  7.31257081e-04  8.70382006e-04  7.50959502e-04\n -4.06643230e-04  5.25340554e-04  1.45635553e-04 -7.93376181e-04\n  9.45130247e-04 -4.62406693e-04  6.12295640e-04 -4.94050328e-04\n -9.15961806e-04 -3.38633195e-04 -9.83705278e-04 -1.02422619e-05\n  5.08910685e-04 -3.24503162e-05  2.58092419e-04  4.73885855e-04\n  5.18614659e-04 -7.14321272e-04 -5.10900922e-04 -6.21284067e-04\n -8.04094656e-04 -1.17780444e-04 -7.51691929e-04 -3.86867090e-04\n  8.39625020e-04 -3.17469880e-04 -7.22947821e-04  1.90507970e-04\n  2.76463426e-04  5.79775311e-04  4.47285303e-04  3.51675466e-04\n -2.07406483e-04  1.35020251e-04  1.91796033e-04  2.40800102e-04\n  1.02115802e-04 -3.34844546e-04  4.13948786e-04 -1.04873371e-03\n  2.78124644e-05 -8.27698619e-04  7.89368176e-04 -5.56862469e-05\n  1.72108412e-04 -5.85200149e-04 -2.41764094e-04 -9.44254396e-04\n -5.30610050e-05  2.01903560e-04 -2.93971214e-04 -8.30941368e-04\n -9.43837047e-04 -2.71706580e-04 -6.15117897e-04  7.63422460e-04\n  1.16879528e-04 -4.24283557e-04  5.06468990e-04  2.99731939e-04\n  9.24136722e-04 -5.27027645e-04  2.20224509e-04  2.03025789e-04\n -8.51639430e-04 -8.31745565e-04 -7.13334652e-04 -1.79207331e-04\n -1.24000420e-04  3.93067923e-04  6.86928281e-04 -4.12007057e-06\n -6.05259091e-04  5.88311232e-04  6.85741543e-05 -2.58440996e-04\n  4.02993377e-04  1.37782408e-04 -4.30044020e-04  2.75345956e-04\n  3.64777283e-04 -1.29056440e-04 -1.17919211e-04 -1.01988122e-03\n -6.13813871e-04 -2.56466737e-04  4.33602429e-04  6.03406806e-04\n -2.96001759e-04  4.85701661e-04  1.73361885e-04 -2.39015673e-04\n -6.15873723e-05  6.73391100e-04  9.58041695e-04  6.61710394e-04\n -4.50587570e-04  8.19916662e-04 -1.12328416e-04  6.69803238e-04\n  5.62291476e-04  8.15727864e-04  3.37521662e-04  4.35934082e-04\n -4.24066529e-04 -3.69222544e-04  9.67476517e-04  5.55943348e-04\n  6.70996786e-04  7.56991911e-04 -9.63597384e-04  4.40350763e-04\n  9.75772506e-04  4.05282248e-04 -6.00215571e-04  6.76497351e-04\n -1.17197436e-04  7.67722435e-04  2.60725938e-04 -9.57938493e-04\n  8.23319715e-05  5.06215962e-04  9.34328069e-04  1.04894309e-04\n -6.91853347e-04  3.34995857e-04  8.92887183e-04  7.75995548e-04\n  2.66402640e-04  1.13916467e-04  5.49024087e-04  1.01374649e-03\n -9.04846005e-04  4.27413528e-04 -2.78779306e-04 -5.65815077e-04\n -9.41482605e-04 -4.59287869e-04 -8.31994344e-04 -4.18681942e-04\n -4.87696729e-04 -2.86486931e-04 -7.33724621e-04  1.42084260e-04\n  4.14178998e-04  7.56158624e-05 -2.05583652e-04 -4.92566265e-04\n -2.24187213e-04 -8.71509721e-04  7.74363056e-04 -2.55352497e-04\n -1.02240976e-03 -2.11339400e-04 -1.10298824e-04 -4.22844430e-04\n -5.56905652e-05  5.41278976e-04 -1.15764407e-04  6.25168264e-04\n  8.60207147e-05 -8.66666611e-04 -9.07445734e-04 -2.07655161e-04\n  5.08484736e-05 -9.90738510e-04 -4.64547978e-04  8.30719582e-05\n -2.90894561e-04  2.85789632e-04 -7.96769600e-05  1.16282201e-04\n -1.29391396e-04  2.52101891e-04  6.44432206e-04  1.56781767e-04\n  1.21857665e-04  1.96659181e-04  8.05938616e-04 -4.34895890e-04\n  5.15392516e-04  7.39732466e-04 -9.83445207e-04 -7.41069671e-04\n  5.19682188e-04  8.50116368e-04 -3.22501379e-04 -2.18313318e-04\n  4.14775568e-05  9.56026837e-04 -6.63060870e-04  5.31114056e-04\n -3.70222289e-04  4.02025558e-04  4.94412729e-04 -5.14229527e-04\n  1.43234138e-04 -3.26509034e-04 -2.94399826e-04  4.53754707e-04\n -4.50294960e-04 -5.68194955e-04 -3.26848734e-04  8.15258303e-04\n  2.92443350e-04 -4.61939781e-04  9.15614422e-04 -7.05072714e-04\n -8.80951295e-04 -3.61389597e-04  7.37599185e-05  1.04767096e-03\n  1.48001011e-04  3.53051844e-04 -1.68611194e-04 -5.97359147e-04\n -6.30282215e-04 -1.54547321e-04 -2.31335769e-04  5.99657884e-04\n -7.65096222e-04  5.07709978e-04  4.53573652e-04  1.68291604e-04\n -3.07933224e-04  3.10915755e-04  9.61436774e-04 -4.91897154e-05\n -8.19992274e-04 -6.57919096e-04  7.45006953e-04 -7.38634146e-04\n  6.16312376e-04 -2.52646714e-04 -3.07291222e-04  1.46183738e-04\n  3.93282971e-04  3.09354480e-04  6.05553272e-04  5.09239850e-04\n -1.13573449e-04  8.46972631e-04  5.20561753e-06 -6.07785361e-04\n  5.56911167e-04 -2.43050439e-04  4.65917255e-04  6.26439578e-04\n  2.58376414e-04 -7.45888683e-04  6.53439609e-04 -7.83204334e-04\n  4.69931692e-04 -9.16473859e-04 -7.34205998e-04 -6.16875244e-04\n  3.37728648e-04 -6.85975538e-04 -4.74461558e-04  3.67608212e-04\n  6.14138320e-04  3.11488780e-04 -3.86318716e-04  3.69041198e-04\n  2.84067413e-04 -1.45758211e-04  9.75367788e-04 -3.25783330e-04\n  3.14242207e-04  2.91316974e-04 -5.02859242e-04 -1.02585368e-03\n -1.77101625e-04 -6.59807061e-04 -9.28662252e-04 -8.60468368e-04\n -1.68298584e-07  9.12376156e-04  5.39395667e-04 -5.36150765e-04\n  9.52610397e-04  4.09700617e-04  2.00936440e-04 -1.02434249e-03\n -2.08581750e-05 -6.62070292e-04  2.76233943e-04  5.06009557e-04\n -1.38003496e-04 -6.99386466e-04 -1.07218357e-05  5.51254547e-04\n -3.32648429e-04  3.11667041e-04 -6.19827129e-04 -8.39974615e-04\n  2.82055058e-04  5.55613078e-04 -6.25714601e-04  2.25211625e-04\n  6.02046435e-04 -4.67319886e-04 -7.80587841e-04  5.97774524e-05\n -9.94371250e-04  4.69317340e-04  9.34413169e-04  4.50655425e-05\n -4.61742282e-04  4.23970807e-04  6.31962263e-04  7.74602988e-04\n  7.16855167e-04  4.26139915e-04 -4.36452858e-04  2.12605184e-04\n -4.04818275e-04 -9.03475622e-04 -9.68386827e-04 -9.96363233e-05\n  9.97451250e-04 -3.79077275e-04 -3.15096986e-06 -1.84747449e-04\n  4.85403172e-04 -7.09226879e-04 -5.18980611e-04  9.28649388e-04\n  2.14630229e-04 -6.83373422e-04  6.82014681e-04  3.65036074e-04\n -4.03588550e-04 -7.18311814e-04 -4.44950361e-04 -5.67709561e-04\n  8.39558837e-04 -9.45342588e-04 -3.16122314e-05  6.09212380e-04\n -5.31068188e-04 -8.00761427e-06  7.37231167e-04  7.84780132e-04\n  8.55251623e-04 -1.02797349e-03  1.02771982e-03  4.52182314e-04\n  6.27746223e-04  9.89818480e-04  1.03576540e-03 -4.87080106e-04\n -6.89347726e-05 -3.51695402e-04  2.49116361e-04  6.67592569e-04\n -1.11513007e-04 -5.30840538e-04 -3.61178507e-04  6.79528806e-04\n  2.38859910e-04  5.25717624e-04 -7.17025192e-04 -1.27804800e-04\n  1.97124493e-04  8.16803309e-04 -5.22120215e-04 -8.05023126e-04\n -6.99451077e-04 -6.96316303e-04 -3.38247890e-04 -8.79391504e-04\n -8.90768541e-04 -1.55949456e-04  1.37793759e-04 -1.02161465e-03\n -5.34893072e-04 -8.30484903e-04  4.07227635e-04  5.51454083e-04\n  8.16712156e-04 -4.47037863e-04  3.61913728e-04 -6.52012997e-04\n  2.31885933e-04 -5.43285045e-04  6.28821843e-04 -6.50013739e-04\n -4.32530680e-04  6.86398358e-04  4.55478817e-04 -3.45295906e-04\n -3.06669128e-04 -9.10373637e-04 -2.23275827e-04 -3.38311307e-04\n  2.78175168e-04  7.82686460e-04 -1.95047047e-04  3.62660590e-04\n -8.27522716e-04 -3.28214883e-05  7.64698023e-04 -5.45713119e-04\n  7.82960851e-04  6.32442359e-04  4.56091948e-04 -6.78037119e-04\n  6.04215369e-04  9.25906701e-04  2.78390682e-04 -8.67383089e-04\n -3.01260559e-04 -6.62381237e-04 -9.00860818e-04 -6.10294082e-05\n -4.01196128e-04  8.53616162e-04  4.67079954e-04  2.06690791e-04\n -7.26288301e-04 -5.90277312e-04  2.52488535e-04  6.93810929e-04\n -4.30558983e-04 -8.74119636e-04 -8.66348739e-04 -7.98222376e-04\n -3.01428314e-04 -2.22055533e-04  8.94460100e-05 -7.90775346e-04\n  5.36696389e-05 -7.59433256e-04 -1.20969944e-05  8.96175043e-04\n  2.91867007e-04  4.74528701e-04  9.13867785e-04 -6.26967463e-04\n -3.31805815e-04 -5.06208045e-04  2.47471384e-04  6.00983796e-04\n -6.03191031e-04  2.84802460e-04 -9.90892731e-05  2.68536125e-04\n -6.40179787e-04  9.07262438e-04 -5.87707444e-04 -8.78518913e-04\n -9.17243306e-04  4.72679880e-04 -6.72219379e-04 -3.47646623e-04\n -5.73344121e-04 -7.32606277e-04  5.55394043e-04 -7.39207026e-04\n -4.29111737e-04 -1.90061983e-04 -5.61318011e-04 -4.19842472e-05\n  4.20255063e-04 -8.93816614e-05 -9.27860732e-04 -7.31159060e-04]\n"
    }
   ],
   "source": [
    "print(word2vec_model[u'文档'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 word2vec词向量模型的保存与调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "word2vec_model.save( 'word2vec_model.w2v' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "word2vec_model = Word2Vec.load( 'word2vec_model.w2v' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 语料向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 每条语料转化为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getVector(wordlist, word2vec_model):\n",
    "    vector_list = [ word2vec_model.wv[k] for k in wordlist if k in word2vec_model]\n",
    "    wordVector = np.array(vector_list).mean(axis = 0)\n",
    "    return wordVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "语料向量形成使用时间为： 0.09478211402893066\n"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "starttime = time.time()\n",
    "contentVector = []\n",
    "for i in range(len(wordlist)):\n",
    "    word = wordlist[i]\n",
    "    usedTime = time.time() - starttime\n",
    "    contentVector.append(getVector(word, word2vec_model))\n",
    "print('语料向量形成使用时间为：',usedTime)\n",
    "X = np.array(contentVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 使用ndarray对象的dump方法保存文章向量化结果X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dump('./datasets/todo_X.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 标签编码转换为矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "y = labelEncoder.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(130, 500) (130,)\n"
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 把X，y拆分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\n"
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_test is     [0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\nLR_predict is [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nLogistic Regression Accurary Is 0.48485\n"
    }
   ],
   "source": [
    "# 1）Logistic Regression 模型\n",
    "def LR_Classify(X_train,y_train):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(penalty = 'l2', max_iter = 10000)\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "LR_model = LR_Classify(X_train,y_train)\n",
    "LR_predict = LR_model.predict(X_test)\n",
    "\n",
    "print('y_test is    ', y_test)\n",
    "print('LR_predict is', LR_predict)\n",
    "LR_model.score(X_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "LR_accuracy = metrics.accuracy_score(y_test, LR_predict)\n",
    "print(\"Logistic Regression Accurary Is %.5f\" %float(LR_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_test is     [0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\nRF_predict is [0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0]\nRandom Forest Accurary Is 0.63636\n"
    }
   ],
   "source": [
    "# 2）Random Forest 模型\n",
    "def RF_Classifier(x_train, y_train):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_estimators=8)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "RF_model = RF_Classifier(X_train,y_train)\n",
    "RF_predict = RF_model.predict(X_test)\n",
    "print('y_test is    ', y_test)\n",
    "print('RF_predict is', RF_predict)\n",
    "RF_model.score(X_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "RF_accuracy = metrics.accuracy_score(y_test, RF_predict)\n",
    "print(\"Random Forest Accurary Is %.5f\" %float(RF_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_test is      [0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\nSVM_predict is [0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\nSVM Accurary Is 0.93939\n"
    }
   ],
   "source": [
    "# 3）SVM 模型\n",
    "def SVM_Classifier(x_train, y_train):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(C=10,kernel='rbf')\n",
    "    model.fit(x_train, y_train)  \n",
    "    return model\n",
    "\n",
    "SVM_model = SVM_Classifier(X_train,y_train)\n",
    "SVM_predict = SVM_model.predict(X_test)\n",
    "print('y_test is     ', y_test)\n",
    "print('SVM_predict is', SVM_predict)\n",
    "SVM_model.score(X_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "SVM_accuracy = metrics.accuracy_score(y_test, SVM_predict)\n",
    "print(\"SVM Accurary Is %.5f\" %float(SVM_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 输入测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    '周二就是发布会了，小王，你今天下午就把演示PPT发给我',\n",
    "    '你们产品评审做到哪一步了，今天下午能不能把方案定稿出来',\n",
    "    '这外卖这么难吃给个差评不过分吧',\n",
    "    '王总购买我们的产品尾款还一直没有付，小李你去催一下看看下个月初能不能结清',\n",
    "    '你们现在效率不太行啊，上班集中一点注意力，今天下午把你们拖一周的产品文案写完发到我这',\n",
    "    '现在上学也太难了',\n",
    "    '公司是一家专注于民生领域信息化的高新技术企业，公司的主营业务为医疗卫生、民政等民生领域的软件开发及硬件销售、技术服务业务，主要面向各级医疗卫生行政管理机构、医院、社区卫生服务中心、新农合、民政行政管理机构等领域的客户，提供软件产品及整体解决方案。公司2013年、2014年、2015年1-10月的营业收入分别为33,681,276.13元、48,088,344.70元、40,972,648.75元。其中，主营业务收入占比分别为99.72%、99.63%、99.64%，主营业务突出。自公司成立以来，主营业务未发生重大变化。',\n",
    "    '我感觉我就是背着电脑换了个地方干和我在产业楼干的一样的事情',\n",
    "    '老王，你下周三把这个程序问题解决掉，不要再让我问第三次',\n",
    "    '周日之前必须把发布需要用的样本做完，不然不够发布的出货',\n",
    "    '别说废话了，再说撕烂你的嘴'\n",
    "]\n",
    "# [1 1 0 1 1 0 0 0 1 1 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 将输入数据转换为X矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextToX(text,word2vec_model):\n",
    "    textwords = []\n",
    "    for i in range(len(text)):\n",
    "        sentence = text[i]\n",
    "        words = [k for k in jieba.cut(sentence) if k not in ['，','。','！','？','、']]\n",
    "        textwords.append(words)\n",
    "    textwords_Vec = []\n",
    "    for i in range(len(textwords)):\n",
    "        textword = textwords[i]\n",
    "        textwords_Vec.append(getVector(textword, word2vec_model))\n",
    "    X_text = np.array(textwords_Vec)\n",
    "    return X_text\n",
    "\n",
    "X_text = TextToX(text,word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 传入预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_text is     [1 1 0 1 1 0 0 0 1 1 0]\nRF_predict is [1 1 0 0 1 1 0 0 1 1 0]\n"
    }
   ],
   "source": [
    "RF_predict = RF_model.predict(X_text)\n",
    "print('y_text is     [1 1 0 1 1 0 0 0 1 1 0]')\n",
    "print('RF_predict is',RF_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}