{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598947486616",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   label                                       text\n0      1    小张，你明天需要把产品方案输出，然后给小王。小王后天把交互做一下，大后天评审。\n1      1                            我明天应该可以把这个文档搞定。\n2      1  统计异常数据那么简单的事情，怎么可能要3天时间呢！今天下班前，务必把这个事情搞定！\n3      1             李总说的这个想法，一定要在8月10日之前形成一个成熟的方案。\n4      1                  走廊里的空箱子太多了，小张你明天把它们全部处理掉。",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>小张，你明天需要把产品方案输出，然后给小王。小王后天把交互做一下，大后天评审。</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>我明天应该可以把这个文档搞定。</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>统计异常数据那么简单的事情，怎么可能要3天时间呢！今天下班前，务必把这个事情搞定！</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>李总说的这个想法，一定要在8月10日之前形成一个成熟的方案。</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>走廊里的空箱子太多了，小张你明天把它们全部处理掉。</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./datasets/todo_classification.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(130, 2)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "有意义文本：65\n无意义文本：65\n"
    }
   ],
   "source": [
    "print('有意义文本：%d' %data[data.label==1].shape[0])\n",
    "print('无意义文本：%d' %data[data.label==0].shape[0])\n",
    "# for name,group in data.groupby(data.columns[0]):\n",
    "#     print(name,len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 使用jieba库制作分词列表wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/vz/l8r_5knx1g3c4yz2yq3mmjnw0000gn/T/jieba.cache\nLoading model cost 3.747 seconds.\nPrefix dict has been built successfully.\n文本分词使用时间： 3.879559278488159\n"
    }
   ],
   "source": [
    "import jieba\n",
    "import time\n",
    "\n",
    "wordlist = []\n",
    "startT = time.time()\n",
    "for row in data['text']:\n",
    "    # splitrow = jieba.cut(row,True)\n",
    "    words = [i for i in jieba.cut(row,True) if i not in ['，','。','！','？','、','.']]\n",
    "    wordlist.append(words)\n",
    "print('文本分词使用时间：',time.time()-startT)\n",
    "# print(wordlist[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 保存分词结果至本地txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtFilepath = './datasets/todotrain.txt'\n",
    "with open(txtFilepath,'w',encoding='utf-8') as fp:\n",
    "    for sentence in wordlist:\n",
    "        fp.write(' '.join(sentence))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 从本地txt文件加载分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtFilepath = './datasets/todotrain.txt'\n",
    "with open(txtFilepath,'r',encoding='utf-8') as fp:\n",
    "    wordlist = [k.strip().split(' ') for k in fp.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['小张', '你', '明天', '需要', '把', '产品', '方案', '输出', '然后', '给', '小', '王', '小', '王后', '后天', '把', '交互', '做', '一下', '大后天', '后天', '评审'], ['我', '明天', '应该', '可以', '把', '这个', '文档', '搞定']]\n"
    }
   ],
   "source": [
    "print(wordlist[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. word2vec词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 实例化word2vec对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "形成word2vec模型共花费1.68秒\n"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "startTime = time.time()\n",
    "word2vec_model = Word2Vec(wordlist, size=500, iter=10, min_count=1)\n",
    "# sentences = wordlist\n",
    "# sentences：可以是一个list，对于大语料集，建议使用BrownCorpus，Text8Corpus或lineSentence构建\n",
    "# size：是指特征向量的维度，默认为100。大的size需要更多的训练数据，但是效果会更好，推荐值为几十到几百\n",
    "# min_count：可以对字典做截断，词频少于min_count次数的单词会被丢弃掉, 默认值为5\n",
    "usedTime = time.time() - startTime\n",
    "print('形成word2vec模型共花费%.2f秒' %usedTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 通过word2vec对象的most_similar方法获取词义相近的次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('的', 0.6049918532371521),\n ('公司', 0.5968180298805237),\n ('一下', 0.5958437919616699),\n ('等', 0.5909460783004761),\n ('为', 0.5864100456237793),\n ('有', 0.5846232771873474),\n ('月', 0.5777654647827148),\n ('我', 0.5730652213096619),\n ('行业', 0.5712067484855652),\n ('和', 0.5682703256607056)]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('周一')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 5.61756839e-04  1.16959738e-04 -9.87790176e-04  6.63977058e-04\n  7.32941320e-04 -4.13161877e-04  3.69930567e-05  4.47949045e-04\n  9.05527500e-04 -7.61229428e-04  5.14284766e-04 -5.21588023e-04\n  8.61329318e-04  6.65424392e-04  7.55101442e-04  5.96803613e-04\n -4.11115121e-04  2.15845546e-04  7.77378271e-04  1.31619337e-04\n -5.20858099e-04  5.22769871e-04  3.35594668e-04  5.20189642e-04\n -9.67365922e-04 -6.66490116e-04 -8.21034890e-04 -1.46806793e-04\n  9.01843887e-05  1.02256867e-03  3.37564648e-04 -4.36384784e-04\n -6.35581382e-04 -5.78270759e-04  1.54905691e-04 -2.07292633e-05\n  3.63018538e-04 -8.49527132e-05 -6.13602970e-05  3.19443905e-04\n  3.17792961e-04  7.89852988e-04 -3.75509087e-04 -1.20603159e-04\n  7.58316251e-04 -2.02676543e-04 -9.97727737e-04  9.22703766e-04\n -2.51987018e-04 -5.21067006e-04  7.33412046e-04 -6.36105775e-04\n  9.29108937e-04 -1.17426513e-04  1.84606048e-04 -4.87781363e-04\n -8.43831396e-04 -3.33187985e-04  1.82192161e-04  1.53232744e-04\n  4.27756837e-04 -5.49746794e-04  6.01485139e-04  3.00243100e-05\n -4.73706372e-04 -1.26763829e-04 -8.60010972e-04 -8.34456063e-04\n  8.67929542e-04 -8.68133677e-04  9.87621723e-04  6.97470270e-04\n -3.20764957e-04 -4.96955414e-04  5.43482311e-04  8.41465313e-04\n -2.83411529e-04  7.62156269e-04 -6.79453195e-04 -6.19887374e-04\n  4.46195590e-05 -7.71802268e-04 -7.47128332e-04 -6.88273693e-04\n  2.63874244e-04 -2.50815385e-04  9.70006105e-04 -7.28332729e-04\n  7.75376800e-04  2.99456791e-04  5.01714007e-04  8.86105467e-04\n  9.40183818e-04 -4.78533882e-04 -7.43239012e-04  4.96961875e-04\n -1.00161927e-03 -9.47496563e-04  1.80081857e-04  2.54716142e-04\n  4.76055604e-04  9.43423714e-04 -4.50025072e-06 -7.38765288e-04\n -8.18439177e-04  8.33564263e-04 -9.14041666e-05  5.79669839e-04\n -1.83206022e-04  6.28386915e-04  2.41004709e-05  7.39113835e-04\n  7.25155114e-04 -3.86092812e-04  2.11362931e-04  7.70572573e-04\n  9.30893177e-04  6.64455001e-04 -7.28393788e-04 -1.00357819e-03\n  3.07930459e-04  7.60735013e-04  2.22094095e-04  8.15520936e-04\n  3.45570996e-04  1.37763040e-04  6.97296637e-05 -2.11649007e-04\n -7.37348048e-04  7.68957427e-04  9.19050246e-04 -7.69595033e-04\n -8.02845054e-04  8.48132535e-04 -7.43232085e-04  1.00595842e-03\n -2.18638306e-04  6.06488553e-04  2.58228305e-04 -8.16443586e-04\n  6.49262278e-04  5.45689545e-04  7.18318915e-04 -7.09221291e-04\n  1.29253312e-04  8.60973785e-04  1.98231392e-05 -4.08493535e-04\n  8.84675886e-04  6.01776701e-05 -3.18170903e-04  5.78090432e-04\n  9.49148263e-04  6.42654486e-05  8.38846317e-04 -9.22828563e-04\n -7.06000137e-04 -7.98927387e-04 -6.44536107e-04  5.81561821e-04\n -8.70238815e-04  1.07409243e-04 -2.29659854e-04 -7.90776801e-04\n  1.02181011e-03 -1.60251439e-04  5.40013389e-05  9.55758369e-05\n  5.57013147e-04 -6.06220216e-04  3.02258675e-04 -7.80319795e-04\n  3.21233441e-04  5.91600721e-04  1.82453325e-04 -9.28178546e-04\n  4.13518195e-04  1.66721056e-05 -2.97486677e-05 -6.27607049e-04\n -2.96946062e-04 -8.78672436e-05 -1.55004353e-04 -6.08204398e-04\n  2.53331265e-04 -4.42218239e-04  5.69108815e-04  8.95149249e-04\n -4.90946812e-04 -8.69762443e-04 -8.96318990e-04  4.07928455e-04\n  3.68330162e-04 -2.82168505e-04  9.53101320e-04  6.03985798e-04\n  5.72467921e-04  5.31918195e-04 -5.82926790e-04  7.80015951e-04\n  5.11939470e-05  9.31307382e-04 -1.03201764e-03 -5.83771325e-04\n  9.25560133e-04 -4.28087369e-04  4.37700422e-04  1.52819412e-04\n -1.60162977e-04 -6.98580639e-04 -9.65961546e-04  6.65133528e-04\n  7.57286907e-04 -6.34353783e-05 -2.84619920e-04  9.32660361e-04\n  3.46644956e-04 -5.39749279e-04  2.37599481e-04 -7.94507941e-05\n -4.58385883e-04 -9.43188032e-04 -3.07486713e-04  5.90498734e-04\n  7.98610854e-04  4.93598753e-04 -3.60624632e-04 -4.72721586e-04\n  4.18837793e-04  1.63651246e-04 -4.12787369e-04 -6.31605726e-05\n  5.85510978e-04 -5.49473625e-04  4.13470145e-04  7.90484075e-04\n -4.57558606e-04  6.33209012e-04 -4.41000535e-04 -9.68075416e-04\n  6.57388358e-04 -6.19692582e-06 -9.34654556e-04  4.12227346e-05\n  5.48827869e-04  3.55935481e-04  1.16863979e-04  4.47541999e-04\n  6.24382577e-04 -2.94521102e-04  2.98533763e-04  8.72806850e-05\n  8.52996192e-04  2.99961685e-04 -5.21684065e-04 -5.10151476e-05\n  1.88398946e-04 -9.26736044e-04 -3.96173680e-04 -5.65810944e-04\n -1.81921587e-05  6.84522674e-04  4.98386449e-04 -7.46497768e-04\n  5.78013191e-04 -3.24536697e-04 -1.01091166e-03  5.51951507e-06\n  7.81381794e-04  1.62075783e-04  7.80150731e-05 -2.43966540e-04\n -3.59963742e-05  9.50353162e-04 -9.87993553e-04 -4.75487992e-04\n -2.05102682e-04  3.88487853e-04 -7.48796010e-05 -2.05203207e-04\n  5.52613230e-04 -8.12270446e-04  6.36094250e-04 -7.70534214e-04\n -1.08344639e-05  5.90406882e-04  1.69833118e-04  2.56817584e-04\n -1.18582560e-04 -9.25948727e-04  4.43101744e-04 -5.96054597e-05\n  8.61941895e-04  5.63092297e-04  1.70855303e-04  1.74900211e-04\n  2.42723429e-04 -4.76659363e-04  6.68339140e-04 -5.95701393e-04\n  7.86673860e-04  9.25845874e-04 -4.27950057e-04 -8.18240573e-04\n  8.82852473e-04 -4.99918358e-04  1.90089864e-04 -1.92402251e-04\n -1.11271896e-04  8.69553245e-04 -2.92118530e-05  1.61937423e-04\n -9.68282926e-04 -4.30890359e-04  1.45643382e-04  5.87026123e-04\n  3.15009878e-04  6.58268749e-04 -6.96661940e-04  7.67939142e-04\n  5.22821560e-04 -6.28712412e-04  3.97495838e-04  8.06585769e-04\n -9.62681428e-04  8.69627052e-04  8.88248091e-04 -6.98439078e-04\n  3.69609304e-04 -9.48680448e-04 -3.22064880e-04  9.39516176e-04\n  6.25704764e-04 -4.47656232e-04 -3.59829748e-04  5.82121953e-04\n -3.50576360e-04 -8.73749494e-04 -7.44368357e-04  1.39317432e-04\n  8.84974608e-04 -3.67056055e-04  9.10201168e-04 -2.66992167e-04\n -9.60621750e-04  6.36437442e-04 -1.13073496e-04  7.99416041e-04\n  1.11829992e-04 -4.60003284e-05  4.92501364e-04 -5.65880327e-04\n -7.92979990e-05  8.67667783e-04 -8.62766348e-04  8.76837526e-04\n  4.18110321e-05 -2.23877487e-05  3.66638356e-04  6.13157463e-04\n -6.71012211e-04 -9.03928536e-04  8.67635710e-04  7.29633495e-04\n  7.52172782e-04  2.55085743e-05  3.16037855e-04  4.47644881e-04\n  2.93594407e-04 -4.45462647e-04  1.40240663e-04 -6.62096892e-04\n  2.67063617e-04 -8.76053353e-04  4.15078917e-04  8.52928380e-04\n  9.09553317e-04 -2.40728623e-05  5.32804370e-05 -6.72076712e-04\n  4.58471623e-04 -5.28898207e-04 -3.69735761e-04 -6.34450233e-04\n  5.85151894e-04  1.65801903e-04  1.16777555e-04  3.61737068e-04\n -7.54554931e-04 -8.99733335e-04  2.57927662e-04  7.46232981e-04\n -3.72441194e-04 -1.66104001e-04  9.84668848e-04 -1.76492744e-04\n -1.83372704e-05  9.11151292e-04  2.40031673e-04  5.16472675e-04\n  5.85365458e-04  8.11271952e-04  5.16914879e-04 -5.42512629e-04\n  4.93978325e-04 -5.19833819e-04 -8.28923134e-04  2.80622946e-04\n  9.35405202e-04  6.01796200e-04  2.49973673e-05 -3.94505274e-04\n -4.36729635e-04  8.28945951e-04 -4.55504312e-04 -8.42592446e-04\n -6.30737282e-04  9.21766448e-04  4.91888903e-04  8.63694353e-04\n -6.07562470e-05 -4.39110045e-05 -3.84672894e-04 -1.10129033e-04\n -9.08064190e-04  1.03351398e-04 -8.62081943e-04 -2.64437404e-04\n -9.61862970e-04 -1.75680383e-04 -7.04033533e-04  9.59555153e-04\n -4.03738115e-04  8.76208433e-05 -5.31626341e-04 -7.39499403e-04\n -9.61451151e-04 -6.19620027e-04  2.09851733e-05  2.75202707e-04\n  8.04388837e-04  8.84933281e-04 -6.36878656e-04 -2.21153110e-04\n  6.11960539e-04  8.38961336e-04 -3.62448394e-04 -8.32602789e-04\n  2.68238422e-04  1.95108645e-04 -3.01845837e-04 -1.95784902e-04\n -4.03521291e-04  3.03149747e-04 -6.94592600e-04 -1.23233913e-05\n  3.50924296e-04  6.44514745e-04  1.53237386e-04 -8.01850169e-04\n  2.98105355e-04 -8.63258727e-04  9.30238399e-04 -9.51526919e-04\n  3.05527705e-04  8.85473099e-04 -5.24192816e-04  5.73355006e-04\n -8.05526157e-04  1.88124512e-04 -7.64780503e-04 -5.16750151e-04\n  7.44181103e-04  4.81807248e-04  2.70178367e-04  1.54058507e-04\n  4.40913427e-04 -8.33423343e-04  2.24893229e-04  5.25848118e-05\n  4.52369801e-04  9.07976064e-04 -6.16683043e-04  3.28318798e-04\n -2.92313460e-04 -4.22987941e-04 -5.42959860e-05 -9.94759961e-04\n -4.92626896e-05  7.81557930e-04  3.74495779e-04 -9.43821913e-04\n  8.91748001e-04  8.06093332e-04  6.93966751e-04 -8.86874695e-05\n  2.78392865e-04 -8.76529957e-05 -2.02670228e-04  2.67328025e-04]\n"
    }
   ],
   "source": [
    "print(word2vec_model[u'文档'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 word2vec词向量模型的保存与调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "word2vec_model.save( 'word2vec_model.w2v' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "word2vec_model = Word2Vec.load( 'word2vec_model.w2v' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 语料向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 每条语料转化为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getVector(wordlist, word2vec_model):\n",
    "    vector_list = [ word2vec_model.wv[k] for k in wordlist if k in word2vec_model]\n",
    "    wordVector = np.array(vector_list).mean(axis = 0)\n",
    "    return wordVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "语料向量形成使用时间为： 0.18552803993225098\n"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "starttime = time.time()\n",
    "contentVector = []\n",
    "for i in range(len(wordlist)):\n",
    "    word = wordlist[i]\n",
    "    usedTime = time.time() - starttime\n",
    "    contentVector.append(getVector(word, word2vec_model))\n",
    "print('语料向量形成使用时间为：',usedTime)\n",
    "X = np.array(contentVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 使用ndarray对象的dump方法保存文章向量化结果X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dump('./datasets/todo_X.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 标签编码转换为矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "y = labelEncoder.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(130, 500) (130,)\n"
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 把X，y拆分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\n"
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_test is     [0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\nLR_predict is [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nLogistic Regression Accurary Is 0.48485\n"
    }
   ],
   "source": [
    "# 1）Logistic Regression 模型\n",
    "def LR_Classify(X_train,y_train):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(penalty = 'l2', max_iter = 10000)\n",
    "    model.fit(X_train,y_train)\n",
    "    return model\n",
    "\n",
    "LR_model = LR_Classify(X_train,y_train)\n",
    "LR_predict = LR_model.predict(X_test)\n",
    "\n",
    "print('y_test is    ', y_test)\n",
    "print('LR_predict is', LR_predict)\n",
    "LR_model.score(X_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "LR_accuracy = metrics.accuracy_score(y_test, LR_predict)\n",
    "print(\"Logistic Regression Accurary Is %.5f\" %float(LR_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_test is     [0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\nRF_predict is [0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0]\nRandom Forest Accurary Is 0.66667\n"
    }
   ],
   "source": [
    "# 2）Random Forest 模型\n",
    "def RF_Classifier(x_train, y_train):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_estimators=8)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "RF_model = RF_Classifier(X_train,y_train)\n",
    "RF_predict = RF_model.predict(X_test)\n",
    "print('y_test is    ', y_test)\n",
    "print('RF_predict is', RF_predict)\n",
    "RF_model.score(X_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "RF_accuracy = metrics.accuracy_score(y_test, RF_predict)\n",
    "print(\"Random Forest Accurary Is %.5f\" %float(RF_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_test is      [0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0]\nSVM_predict is [0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0]\nSVM Accurary Is 0.96970\n"
    }
   ],
   "source": [
    "# 3）SVM 模型\n",
    "def SVM_Classifier(x_train, y_train):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(C=10,kernel='rbf')\n",
    "    model.fit(x_train, y_train)  \n",
    "    return model\n",
    "\n",
    "SVM_model = SVM_Classifier(X_train,y_train)\n",
    "SVM_predict = SVM_model.predict(X_test)\n",
    "print('y_test is     ', y_test)\n",
    "print('SVM_predict is', SVM_predict)\n",
    "SVM_model.score(X_test,y_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "SVM_accuracy = metrics.accuracy_score(y_test, SVM_predict)\n",
    "print(\"SVM Accurary Is %.5f\" %float(SVM_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 输入测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    '周二就是发布会了，小王，你今天下午就把演示PPT发给我',\n",
    "    '你们产品评审做到哪一步了，今天下午能不能把方案定稿出来',\n",
    "    '这外卖这么难吃给个差评不过分吧',\n",
    "    '王总购买我们的产品尾款还一直没有付，小李你去催一下看看下个月初能不能结清',\n",
    "    '你们现在效率不太行啊，上班集中一点注意力，今天下午把你们拖一周的产品文案写完发到我这',\n",
    "    '现在上学也太难了',\n",
    "    '公司是一家专注于民生领域信息化的高新技术企业，公司的主营业务为医疗卫生、民政等民生领域的软件开发及硬件销售、技术服务业务，主要面向各级医疗卫生行政管理机构、医院、社区卫生服务中心、新农合、民政行政管理机构等领域的客户，提供软件产品及整体解决方案。公司2013年、2014年、2015年1-10月的营业收入分别为33,681,276.13元、48,088,344.70元、40,972,648.75元。其中，主营业务收入占比分别为99.72%、99.63%、99.64%，主营业务突出。自公司成立以来，主营业务未发生重大变化。',\n",
    "    '我感觉我就是背着电脑换了个地方干和我在产业楼干的一样的事情',\n",
    "    '老王，你下周三把这个程序问题解决掉，不要再让我问第三次',\n",
    "    '周日之前必须把发布需要用的样本做完，不然不够发布的出货',\n",
    "    '别说废话了，再说撕烂你的嘴'\n",
    "]\n",
    "# [1 1 0 1 1 0 0 0 1 1 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 将输入数据转换为X矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextToX(text,word2vec_model):\n",
    "    textwords = []\n",
    "    for i in range(len(text)):\n",
    "        sentence = text[i]\n",
    "        words = [k for k in jieba.cut(sentence) if k not in ['，','。','！','？','、']]\n",
    "        textwords.append(words)\n",
    "    textwords_Vec = []\n",
    "    for i in range(len(textwords)):\n",
    "        textword = textwords[i]\n",
    "        textwords_Vec.append(getVector(textword, word2vec_model))\n",
    "    X_text = np.array(textwords_Vec)\n",
    "    return X_text\n",
    "\n",
    "X_text = TextToX(text,word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 传入预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y_text is     [1 1 0 1 1 0 0 0 1 1 0]\nRF_predict is [1 1 0 1 1 0 0 0 1 0 0]\n"
    }
   ],
   "source": [
    "RF_predict = RF_model.predict(X_text)\n",
    "print('y_text is     [1 1 0 1 1 0 0 0 1 1 0]')\n",
    "print('RF_predict is',RF_predict)"
   ]
  }
 ]
}